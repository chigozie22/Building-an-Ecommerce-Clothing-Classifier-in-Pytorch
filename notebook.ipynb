{"cells":[{"source":"![clothing_classification](clothing_classification.png)\n","metadata":{},"id":"aaa02648-9eae-45ba-893f-88440e8e4235","cell_type":"markdown"},{"source":"Fashion Forward is a new AI-based e-commerce clothing retailer.\nThey want to use image classification to automatically categorize new product listings, making it easier for customers to find what they're looking for. It will also assist in inventory management by quickly sorting items.\n\nAs a data scientist tasked with implementing a garment classifier, your primary objective is to develop a machine learning model capable of accurately categorizing images of clothing items into distinct garment types such as shirts, trousers, shoes, etc.","metadata":{},"id":"ad5a988c-1095-485a-a88c-002400a872be","cell_type":"markdown"},{"source":"# Run the cells below first","metadata":{"executionCancelledAt":null,"executionTime":47,"lastExecutedAt":1742822367860,"lastExecutedByKernel":"cf3c334b-b11b-4e5d-a104-12582d60e0ad","lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"# Run the cells below first"},"id":"4a1ab317-f3e4-4e5f-93a7-9c27677c5ffb","cell_type":"code","execution_count":17,"outputs":[]},{"source":"!pip install torchmetrics\n!pip install torchvision","metadata":{"executionCancelledAt":null,"executionTime":6032,"lastExecutedAt":1742822373892,"lastExecutedByKernel":"cf3c334b-b11b-4e5d-a104-12582d60e0ad","lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"!pip install torchmetrics\n!pip install torchvision","outputsMetadata":{"0":{"height":445,"type":"stream"}},"collapsed":false,"jupyter":{"outputs_hidden":false,"source_hidden":false}},"id":"93e7dae3-c192-4267-a0ed-18d1ac56c861","cell_type":"code","execution_count":18,"outputs":[{"output_type":"stream","name":"stdout","text":"Defaulting to user installation because normal site-packages is not writeable\nRequirement already satisfied: torchmetrics in /usr/local/lib/python3.10/dist-packages (1.6.1)\nRequirement already satisfied: numpy>1.20.0 in /usr/local/lib/python3.10/dist-packages (from torchmetrics) (1.26.4)\nRequirement already satisfied: packaging>17.1 in /usr/local/lib/python3.10/dist-packages (from torchmetrics) (23.2)\nRequirement already satisfied: torch>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from torchmetrics) (2.3.1)\nRequirement already satisfied: lightning-utilities>=0.8.0 in /usr/local/lib/python3.10/dist-packages (from torchmetrics) (0.12.0)\nRequirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from lightning-utilities>=0.8.0->torchmetrics) (72.2.0)\nRequirement already satisfied: typing_extensions in /usr/local/lib/python3.10/dist-packages (from lightning-utilities>=0.8.0->torchmetrics) (4.12.2)\nRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=2.0.0->torchmetrics) (3.17.0)\nRequirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=2.0.0->torchmetrics) (1.13.3)\nRequirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=2.0.0->torchmetrics) (3.4.2)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=2.0.0->torchmetrics) (3.1.5)\nRequirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=2.0.0->torchmetrics) (2024.9.0)\nRequirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=2.0.0->torchmetrics) (12.1.105)\nRequirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=2.0.0->torchmetrics) (12.1.105)\nRequirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=2.0.0->torchmetrics) (12.1.105)\nRequirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.10/dist-packages (from torch>=2.0.0->torchmetrics) (8.9.2.26)\nRequirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch>=2.0.0->torchmetrics) (12.1.3.1)\nRequirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch>=2.0.0->torchmetrics) (11.0.2.54)\nRequirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch>=2.0.0->torchmetrics) (10.3.2.106)\nRequirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch>=2.0.0->torchmetrics) (11.4.5.107)\nRequirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch>=2.0.0->torchmetrics) (12.1.0.106)\nRequirement already satisfied: nvidia-nccl-cu12==2.20.5 in /usr/local/lib/python3.10/dist-packages (from torch>=2.0.0->torchmetrics) (2.20.5)\nRequirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=2.0.0->torchmetrics) (12.1.105)\nRequirement already satisfied: triton==2.3.1 in /usr/local/lib/python3.10/dist-packages (from torch>=2.0.0->torchmetrics) (2.3.1)\nRequirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=2.0.0->torchmetrics) (12.8.61)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=2.0.0->torchmetrics) (3.0.2)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=2.0.0->torchmetrics) (1.3.0)\nDefaulting to user installation because normal site-packages is not writeable\nRequirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (0.18.1)\nRequirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision) (1.26.4)\nRequirement already satisfied: torch==2.3.1 in /usr/local/lib/python3.10/dist-packages (from torchvision) (2.3.1)\nRequirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision) (11.1.0)\nRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch==2.3.1->torchvision) (3.17.0)\nRequirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch==2.3.1->torchvision) (4.12.2)\nRequirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch==2.3.1->torchvision) (1.13.3)\nRequirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch==2.3.1->torchvision) (3.4.2)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch==2.3.1->torchvision) (3.1.5)\nRequirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch==2.3.1->torchvision) (2024.9.0)\nRequirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch==2.3.1->torchvision) (12.1.105)\nRequirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch==2.3.1->torchvision) (12.1.105)\nRequirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch==2.3.1->torchvision) (12.1.105)\nRequirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.10/dist-packages (from torch==2.3.1->torchvision) (8.9.2.26)\nRequirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch==2.3.1->torchvision) (12.1.3.1)\nRequirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch==2.3.1->torchvision) (11.0.2.54)\nRequirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch==2.3.1->torchvision) (10.3.2.106)\nRequirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch==2.3.1->torchvision) (11.4.5.107)\nRequirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch==2.3.1->torchvision) (12.1.0.106)\nRequirement already satisfied: nvidia-nccl-cu12==2.20.5 in /usr/local/lib/python3.10/dist-packages (from torch==2.3.1->torchvision) (2.20.5)\nRequirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch==2.3.1->torchvision) (12.1.105)\nRequirement already satisfied: triton==2.3.1 in /usr/local/lib/python3.10/dist-packages (from torch==2.3.1->torchvision) (2.3.1)\nRequirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch==2.3.1->torchvision) (12.8.61)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch==2.3.1->torchvision) (3.0.2)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch==2.3.1->torchvision) (1.3.0)\n"}]},{"source":"import numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import Dataset, DataLoader\nfrom torchmetrics import Accuracy, Precision, Recall\nimport torch.nn.functional as F","metadata":{"executionCancelledAt":null,"executionTime":50,"lastExecutedAt":1742822373944,"lastExecutedByKernel":"cf3c334b-b11b-4e5d-a104-12582d60e0ad","lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"import numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import Dataset, DataLoader\nfrom torchmetrics import Accuracy, Precision, Recall\nimport torch.nn.functional as F"},"id":"ea8065b7-84fc-4376-afef-6db731dec4b3","cell_type":"code","execution_count":19,"outputs":[]},{"source":"# Load datasets\nfrom torchvision import datasets\nimport torchvision.transforms as transforms\n\ntrain_transform = transforms.Compose([\n    transforms.RandomHorizontalFlip(),\n    transforms.RandomRotation(45),\n    transforms.Resize((64,64)),\n    transforms.ToTensor(),\n    transforms.Normalize((0.5,), (0.5,))\n])\n\ntest_transform = transforms.Compose([\n    transforms.RandomHorizontalFlip(),\n    transforms.RandomRotation(45),\n    transforms.Resize((64,64)),\n    transforms.ToTensor(),\n    transforms.Normalize((0.5,), (0.5,))\n])\n\ntrain_data = datasets.FashionMNIST(root='./data', train=True, download=True, transform=train_transform)\ntest_data = datasets.FashionMNIST(root='./data', train=False, download=True, transform=test_transform)","metadata":{"executionCancelledAt":null,"executionTime":162,"lastExecutedAt":1742822374106,"lastExecutedByKernel":"cf3c334b-b11b-4e5d-a104-12582d60e0ad","lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"# Load datasets\nfrom torchvision import datasets\nimport torchvision.transforms as transforms\n\ntrain_transform = transforms.Compose([\n    transforms.RandomHorizontalFlip(),\n    transforms.RandomRotation(45),\n    transforms.Resize((64,64)),\n    transforms.ToTensor(),\n    transforms.Normalize((0.5,), (0.5,))\n])\n\ntest_transform = transforms.Compose([\n    transforms.RandomHorizontalFlip(),\n    transforms.RandomRotation(45),\n    transforms.Resize((64,64)),\n    transforms.ToTensor(),\n    transforms.Normalize((0.5,), (0.5,))\n])\n\ntrain_data = datasets.FashionMNIST(root='./data', train=True, download=True, transform=train_transform)\ntest_data = datasets.FashionMNIST(root='./data', train=False, download=True, transform=test_transform)","outputsMetadata":{"0":{"height":101,"type":"stream"},"1":{"height":38,"type":"stream"},"2":{"height":164,"type":"stream"},"3":{"height":38,"type":"stream"},"4":{"height":122,"type":"stream"},"5":{"height":38,"type":"stream"},"6":{"height":164,"type":"stream"},"7":{"height":38,"type":"stream"},"8":{"height":80,"type":"stream"}}},"id":"662e1bf1-943f-4243-9fd4-02ce11609e8d","cell_type":"code","execution_count":20,"outputs":[]},{"source":"# print(len(train_data.classes))\nimage_size = train_data[0][0].shape[1]\n\nclass FashionNet(nn.Module):\n    def __init__(self, num_classes):\n        super(FashionNet, self).__init__()\n        \n        self.conv1 = nn.Conv2d(1, 16, kernel_size=3, stride=1, padding=1)\n        self.relu = nn.ReLU()\n        self.maxpool = nn.MaxPool2d(kernel_size=2, stride=2)\n        self.flatten = nn.Flatten()\n        \n        self.fc1 = nn.Linear(16 * (image_size // 2)**2 , num_classes)\n        \n    def forward(self, x):\n        x = self.conv1(x)\n        x = self.relu(x)\n        x = self.maxpool(x)\n        x = self.flatten(x)\n        x = self.fc1(x)\n        \n        return x","metadata":{"executionCancelledAt":null,"executionTime":49,"lastExecutedAt":1742822374156,"lastExecutedByKernel":"cf3c334b-b11b-4e5d-a104-12582d60e0ad","lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"# print(len(train_data.classes))\nimage_size = train_data[0][0].shape[1]\n\nclass FashionNet(nn.Module):\n    def __init__(self, num_classes):\n        super(FashionNet, self).__init__()\n        \n        self.conv1 = nn.Conv2d(1, 16, kernel_size=3, stride=1, padding=1)\n        self.relu = nn.ReLU()\n        self.maxpool = nn.MaxPool2d(kernel_size=2, stride=2)\n        self.flatten = nn.Flatten()\n        \n        self.fc1 = nn.Linear(16 * (image_size // 2)**2 , num_classes)\n        \n    def forward(self, x):\n        x = self.conv1(x)\n        x = self.relu(x)\n        x = self.maxpool(x)\n        x = self.flatten(x)\n        x = self.fc1(x)\n        \n        return x","outputsMetadata":{"0":{"height":38,"type":"stream"}}},"cell_type":"code","id":"9ca49763-c0b1-4c26-8fa8-3aa4f441db56","outputs":[],"execution_count":21},{"source":"# class FashionNet(nn.Module):\n#     def __init__(self, num_classes):\n#         super(FashionNet, self).__init__()\n        \n#         self.conv1 = nn.Conv2d(1, 32, kernel_size=3, padding=1)\n#         self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n#         self.conv3 = nn.Conv2d(64, 128, kernel_size=3, padding=1)\n#         self.conv4 = nn.Conv2d(128, 512, kernel_size=3, padding=1)\n#         self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n#         self.dropout = nn.Dropout(0.5)\n#         self.fc1 = nn.Linear(512*4*4, 1024)\n#         self.fc2 = nn.Linear(1024, num_classes)\n#         self.batch_norm1 = nn.BatchNorm2d(32)\n#         self.batch_norm2 = nn.BatchNorm2d(64)\n#         self.batch_norm3 = nn.BatchNorm2d(128)\n#         self.batch_norm4 = nn.BatchNorm2d(512)\n        \n#     def forward(self, x):\n#         x = self.pool(F.relu(self.batch_norm1(self.conv1(x))))\n#         x = self.pool(F.relu(self.batch_norm2(self.conv2(x))))\n#         x = self.pool(F.relu(self.batch_norm3(self.conv3(x))))\n#         x = self.pool(F.relu(self.batch_norm4(self.conv4(x))))\n#         x = x.view(-1, 512*4*4)\n#         x = F.relu(self.fc1(x))\n#         x = self.dropout(x)\n#         x = self.fc2(x)\n#         return x\n    \n#     # def _calculate_feature_size(self):\n#     #     with torch.no_grad:\n#     #         dummy = torch.zeros()","metadata":{"executionCancelledAt":null,"executionTime":47,"lastExecutedAt":1742822374204,"lastExecutedByKernel":"cf3c334b-b11b-4e5d-a104-12582d60e0ad","lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"# class FashionNet(nn.Module):\n#     def __init__(self, num_classes):\n#         super(FashionNet, self).__init__()\n        \n#         self.conv1 = nn.Conv2d(1, 32, kernel_size=3, padding=1)\n#         self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n#         self.conv3 = nn.Conv2d(64, 128, kernel_size=3, padding=1)\n#         self.conv4 = nn.Conv2d(128, 512, kernel_size=3, padding=1)\n#         self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n#         self.dropout = nn.Dropout(0.5)\n#         self.fc1 = nn.Linear(512*4*4, 1024)\n#         self.fc2 = nn.Linear(1024, num_classes)\n#         self.batch_norm1 = nn.BatchNorm2d(32)\n#         self.batch_norm2 = nn.BatchNorm2d(64)\n#         self.batch_norm3 = nn.BatchNorm2d(128)\n#         self.batch_norm4 = nn.BatchNorm2d(512)\n        \n#     def forward(self, x):\n#         x = self.pool(F.relu(self.batch_norm1(self.conv1(x))))\n#         x = self.pool(F.relu(self.batch_norm2(self.conv2(x))))\n#         x = self.pool(F.relu(self.batch_norm3(self.conv3(x))))\n#         x = self.pool(F.relu(self.batch_norm4(self.conv4(x))))\n#         x = x.view(-1, 512*4*4)\n#         x = F.relu(self.fc1(x))\n#         x = self.dropout(x)\n#         x = self.fc2(x)\n#         return x\n    \n#     # def _calculate_feature_size(self):\n#     #     with torch.no_grad:\n#     #         dummy = torch.zeros()"},"cell_type":"code","id":"fb75d12e-6b3a-46a9-9a1b-26e79b871bbb","outputs":[],"execution_count":22},{"source":"# # Start coding here\n# # Use as many cells as you need \n\n# class FashionNet(nn.Module):\n#     def __init__(self, num_classes):\n#         super(FashionNet, self).__init__()\n#         self.feature_extractor = nn.Sequential(\n#             nn.Conv2d(1, 32, kernel_size=3, padding=1),\n#             nn.ReLU(),\n#             nn.MaxPool2d(kernel_size=2),\n#             nn.Conv2d(32, 64, kernel_size=3, padding=1),\n#             nn.ELU(),\n#             # nn.Conv2d(64, 128, kernel_size=3, padding=1),\n#             # nn.ReLU(),\n#             # nn.Conv2d(128, 512, kernel_size=3, padding=1),\n#             # nn.ReLU(),\n#             nn.Flatten(),\n#         )\n#         self._calculate_feature_size()\n        \n#         self.classifier = nn.Linear(self.feature_size, num_classes)\n        \n#     def forward(self, x):\n#         x = self.feature_extractor(x)\n#         x = self.classifier(x)\n#         return x\n    \n#     def _calculate_feature_size(self):\n#         # Use a dummy tensor to compute the output size of the feature extractor\n#         with torch.no_grad():\n#             dummy_input = torch.zeros(1, 1, 64, 64)  # Assuming input image size \n#             feature_size = self.feature_extractor(dummy_input).view(1, -1).size(1)\n#             print(feature_size)                                                                   \n#         self.feature_size = feature_size\n    ","metadata":{"executionCancelledAt":null,"executionTime":47,"lastExecutedAt":1742822374252,"lastExecutedByKernel":"cf3c334b-b11b-4e5d-a104-12582d60e0ad","lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"# # Start coding here\n# # Use as many cells as you need \n\n# class FashionNet(nn.Module):\n#     def __init__(self, num_classes):\n#         super(FashionNet, self).__init__()\n#         self.feature_extractor = nn.Sequential(\n#             nn.Conv2d(1, 32, kernel_size=3, padding=1),\n#             nn.ReLU(),\n#             nn.MaxPool2d(kernel_size=2),\n#             nn.Conv2d(32, 64, kernel_size=3, padding=1),\n#             nn.ELU(),\n#             # nn.Conv2d(64, 128, kernel_size=3, padding=1),\n#             # nn.ReLU(),\n#             # nn.Conv2d(128, 512, kernel_size=3, padding=1),\n#             # nn.ReLU(),\n#             nn.Flatten(),\n#         )\n#         self._calculate_feature_size()\n        \n#         self.classifier = nn.Linear(self.feature_size, num_classes)\n        \n#     def forward(self, x):\n#         x = self.feature_extractor(x)\n#         x = self.classifier(x)\n#         return x\n    \n#     def _calculate_feature_size(self):\n#         # Use a dummy tensor to compute the output size of the feature extractor\n#         with torch.no_grad():\n#             dummy_input = torch.zeros(1, 1, 64, 64)  # Assuming input image size \n#             feature_size = self.feature_extractor(dummy_input).view(1, -1).size(1)\n#             print(feature_size)                                                                   \n#         self.feature_size = feature_size\n    "},"id":"53c0a71d-d7d9-4a11-8a9b-55867ea7e0b5","cell_type":"code","execution_count":23,"outputs":[]},{"source":"train_loader = DataLoader(train_data, shuffle=True, batch_size=10)","metadata":{"executionCancelledAt":null,"executionTime":52,"lastExecutedAt":1742822374304,"lastExecutedByKernel":"cf3c334b-b11b-4e5d-a104-12582d60e0ad","lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"train_loader = DataLoader(train_data, shuffle=True, batch_size=10)"},"cell_type":"code","id":"42946f61-1bfe-4abf-8698-f644979da29e","outputs":[],"execution_count":24},{"source":"num_classes = 10\nnet = FashionNet(num_classes=10)\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.Adam(net.parameters(), lr=0.01)\nscheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', patience=5, factor=0.1)\n\ndef train_net(model, train_loader, criterion, optimizer, num_epochs):\n    for epoch in range(num_epochs):\n        running_loss = 0\n        for images, labels in train_loader:\n            optimizer.zero_grad()\n            # image = images.squeeze().permute(1,2,0)\n            outputs = net(images)\n            loss = criterion(outputs, labels)\n            loss.backward()\n            optimizer.step()\n            running_loss += loss.item()\n            \n    train_loss = running_loss / len(train_loader)\n    print(f\"Training loss after {num_epochs} epochs is {train_loss}\")","metadata":{"executionCancelledAt":null,"executionTime":52,"lastExecutedAt":1742822374356,"lastExecutedByKernel":"cf3c334b-b11b-4e5d-a104-12582d60e0ad","lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"num_classes = 10\nnet = FashionNet(num_classes=10)\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.Adam(net.parameters(), lr=0.01)\nscheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', patience=5, factor=0.1)\n\ndef train_net(model, train_loader, criterion, optimizer, num_epochs):\n    for epoch in range(num_epochs):\n        running_loss = 0\n        for images, labels in train_loader:\n            optimizer.zero_grad()\n            # image = images.squeeze().permute(1,2,0)\n            outputs = net(images)\n            loss = criterion(outputs, labels)\n            loss.backward()\n            optimizer.step()\n            running_loss += loss.item()\n            \n    train_loss = running_loss / len(train_loader)\n    print(f\"Training loss after {num_epochs} epochs is {train_loss}\")","outputsMetadata":{"0":{"height":38,"type":"stream"}}},"cell_type":"code","id":"866d47b2-fefb-4a4f-b25a-98114efdc11a","outputs":[],"execution_count":25},{"source":"# train_net(net, train_loader, criterion, optimizer, 5)","metadata":{"executionCancelledAt":null,"executionTime":47,"lastExecutedAt":1742822374404,"lastExecutedByKernel":"cf3c334b-b11b-4e5d-a104-12582d60e0ad","lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"# train_net(net, train_loader, criterion, optimizer, 5)","outputsMetadata":{"0":{"height":38,"type":"stream"}},"collapsed":false,"jupyter":{"outputs_hidden":false,"source_hidden":false}},"cell_type":"code","id":"b407a5e9-73b1-43f5-aa1b-85fb50a4629b","outputs":[],"execution_count":26},{"source":"for images, labels in train_loader:\n    print(images[0].shape)\n    break\n\n# print(len(test_loader))","metadata":{"executionCancelledAt":null,"executionTime":49,"lastExecutedAt":1742822374453,"lastExecutedByKernel":"cf3c334b-b11b-4e5d-a104-12582d60e0ad","lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"for images, labels in train_loader:\n    print(images[0].shape)\n    break\n\n# print(len(test_loader))","outputsMetadata":{"0":{"height":38,"type":"stream"}}},"cell_type":"code","id":"dfae2577-d482-4f47-b28e-189f54c249a2","outputs":[{"output_type":"stream","name":"stdout","text":"torch.Size([1, 64, 64])\n"}],"execution_count":27},{"source":"### Prediction","metadata":{},"cell_type":"markdown","id":"b49edd6f-33ba-476d-93f9-1a1ca592fab5"},{"source":"test_loader = DataLoader(test_data, shuffle=False, batch_size=10)","metadata":{"executionCancelledAt":null,"executionTime":47,"lastExecutedAt":1742822374500,"lastExecutedByKernel":"cf3c334b-b11b-4e5d-a104-12582d60e0ad","lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"test_loader = DataLoader(test_data, shuffle=False, batch_size=10)"},"cell_type":"code","id":"533544d3-fdf2-4508-ad6c-e47d240f2e58","outputs":[],"execution_count":28},{"source":"precision_macro = Precision(task='multiclass', num_classes=10, average='macro')\nrecall_macro = Recall(task='multiclass', num_classes=10, average='macro')\naccuracy_macro = Accuracy(task='multiclass', num_classes=10, average='macro')\n\n\npredictions = []\n\nnet.eval()\nwith torch.no_grad():\n    for i, (images, labels) in enumerate(test_loader):\n        outputs = net.forward(images.reshape(-1, 1, image_size, image_size))\n        preds = torch.argmax(outputs, dim=-1)\n        predictions.extend(preds.cpu().tolist())\n        precision_macro(preds, labels)\n        recall_macro(preds, labels)\n        accuracy_macro(preds, labels)\n    \nprecision = precision_macro.compute()\nrecall = recall_macro.compute()\naccuracy = accuracy_macro.compute()","metadata":{"executionCancelledAt":null,"executionTime":27448,"lastExecutedAt":1742822401948,"lastExecutedByKernel":"cf3c334b-b11b-4e5d-a104-12582d60e0ad","lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"precision_macro = Precision(task='multiclass', num_classes=10, average='macro')\nrecall_macro = Recall(task='multiclass', num_classes=10, average='macro')\naccuracy_macro = Accuracy(task='multiclass', num_classes=10, average='macro')\n\n\npredictions = []\n\nnet.eval()\nwith torch.no_grad():\n    for i, (images, labels) in enumerate(test_loader):\n        outputs = net.forward(images.reshape(-1, 1, image_size, image_size))\n        preds = torch.argmax(outputs, dim=-1)\n        predictions.extend(preds.cpu().tolist())\n        precision_macro(preds, labels)\n        recall_macro(preds, labels)\n        accuracy_macro(preds, labels)\n    \nprecision = precision_macro.compute()\nrecall = recall_macro.compute()\naccuracy = accuracy_macro.compute()"},"cell_type":"code","id":"ea890e4a-b1e0-449a-9baa-e10da31cfb01","outputs":[],"execution_count":29},{"source":"precision = float(precision_macro.compute())\nrecall = float(recall_macro.compute())\naccuracy = float(accuracy_macro.compute())","metadata":{"executionCancelledAt":null,"executionTime":46,"lastExecutedAt":1742822401996,"lastExecutedByKernel":"cf3c334b-b11b-4e5d-a104-12582d60e0ad","lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"precision = float(precision_macro.compute())\nrecall = float(recall_macro.compute())\naccuracy = float(accuracy_macro.compute())"},"cell_type":"code","id":"d93b4b82-4fa8-4515-be1d-828974b2152b","outputs":[],"execution_count":30},{"source":"print(f\"Precision: {precision}\")\nprint(f\"Recall: {recall}\")\nprint(f\"Accuracy: {accuracy}\")","metadata":{"executionCancelledAt":null,"executionTime":48,"lastExecutedAt":1742822402044,"lastExecutedByKernel":"cf3c334b-b11b-4e5d-a104-12582d60e0ad","lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"print(f\"Precision: {precision}\")\nprint(f\"Recall: {recall}\")\nprint(f\"Accuracy: {accuracy}\")","outputsMetadata":{"0":{"height":80,"type":"stream"}}},"cell_type":"code","id":"6c3ca9aa-8e26-4dbd-b760-b383eaff33dd","outputs":[{"output_type":"stream","name":"stdout","text":"Precision: 0.046081073582172394\nRecall: 0.08709999918937683\nAccuracy: 0.08709999918937683\n"}],"execution_count":31},{"source":"","metadata":{"executionCancelledAt":null,"executionTime":52,"lastExecutedAt":1742822402096,"lastExecutedByKernel":"cf3c334b-b11b-4e5d-a104-12582d60e0ad","lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"","outputsMetadata":{"0":{"height":38,"type":"stream"}}},"cell_type":"code","id":"f764cce9-3a64-4f7f-99c7-8a4bb36b42d0","outputs":[],"execution_count":31},{"source":"print(predictions)\npredictions[0:20]","metadata":{"executionCancelledAt":null,"executionTime":13,"lastExecutedAt":1742822402109,"lastExecutedByKernel":"cf3c334b-b11b-4e5d-a104-12582d60e0ad","lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"print(predictions)\npredictions[0:20]","outputsMetadata":{"0":{"height":445,"type":"stream"}}},"cell_type":"code","id":"c36a654c-f3f0-4004-9b71-f11882f703cf","outputs":[{"output_type":"stream","name":"stdout","text":"[2, 2, 2, 2, 2, 6, 2, 2, 2, 2, 2, 2, 2, 2, 2, 6, 2, 2, 2, 9, 2, 2, 2, 2, 2, 6, 2, 9, 2, 2, 2, 8, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 6, 2, 2, 9, 2, 2, 2, 9, 2, 2, 9, 2, 2, 9, 2, 2, 2, 2, 2, 2, 2, 2, 6, 2, 2, 6, 5, 2, 2, 9, 2, 2, 2, 2, 9, 2, 9, 2, 6, 6, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 6, 6, 2, 2, 9, 9, 2, 2, 2, 2, 2, 6, 6, 2, 5, 2, 2, 2, 2, 9, 6, 9, 9, 2, 2, 9, 2, 2, 6, 2, 2, 2, 2, 2, 2, 6, 2, 2, 2, 2, 9, 2, 2, 2, 2, 2, 2, 9, 2, 2, 2, 9, 2, 2, 9, 2, 2, 2, 2, 2, 2, 2, 2, 2, 6, 2, 2, 2, 2, 2, 2, 2, 2, 6, 9, 2, 2, 2, 2, 2, 9, 2, 2, 2, 2, 2, 6, 6, 2, 2, 2, 2, 6, 2, 9, 2, 2, 2, 2, 2, 2, 2, 2, 2, 6, 2, 2, 2, 2, 6, 2, 9, 6, 6, 2, 2, 9, 2, 9, 2, 2, 2, 2, 2, 2, 2, 6, 2, 6, 2, 2, 2, 2, 0, 2, 6, 2, 2, 6, 2, 6, 2, 2, 2, 2, 9, 2, 6, 6, 6, 6, 2, 2, 9, 2, 2, 2, 2, 2, 2, 6, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 9, 2, 2, 2, 2, 2, 9, 2, 9, 2, 2, 2, 2, 6, 2, 9, 2, 2, 9, 2, 2, 2, 2, 2, 6, 2, 2, 2, 2, 2, 9, 2, 2, 2, 2, 9, 9, 2, 2, 2, 2, 2, 2, 2, 2, 6, 2, 2, 6, 2, 2, 2, 2, 6, 2, 2, 6, 2, 2, 2, 2, 2, 2, 2, 2, 9, 2, 2, 2, 2, 9, 2, 2, 9, 2, 2, 2, 2, 2, 2, 2, 2, 6, 2, 2, 2, 2, 2, 2, 2, 2, 2, 9, 2, 9, 5, 2, 2, 2, 2, 9, 9, 6, 2, 6, 6, 2, 2, 2, 2, 2, 2, 2, 2, 9, 2, 2, 2, 2, 2, 2, 2, 2, 6, 2, 2, 6, 6, 2, 9, 6, 2, 2, 2, 2, 2, 2, 2, 6, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 9, 2, 2, 2, 2, 2, 9, 6, 2, 6, 6, 2, 6, 5, 6, 2, 2, 2, 6, 2, 2, 2, 2, 2, 2, 6, 6, 6, 9, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 9, 2, 6, 2, 2, 2, 2, 2, 2, 2, 2, 6, 5, 6, 2, 2, 6, 2, 2, 2, 6, 2, 6, 2, 6, 2, 6, 6, 2, 2, 2, 6, 2, 2, 2, 2, 2, 2, 6, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 9, 2, 9, 2, 2, 2, 2, 6, 8, 2, 2, 2, 2, 2, 9, 2, 6, 6, 2, 2, 2, 9, 2, 2, 2, 6, 9, 6, 2, 2, 6, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 6, 2, 2, 2, 2, 2, 2, 2, 2, 2, 6, 6, 6, 2, 2, 2, 9, 2, 2, 6, 5, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 9, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 6, 2, 2, 2, 6, 6, 2, 2, 2, 2, 6, 2, 2, 2, 2, 2, 2, 2, 6, 2, 2, 2, 2, 2, 2, 2, 2, 9, 2, 2, 2, 2, 2, 2, 2, 2, 9, 9, 2, 2, 6, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 9, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 9, 2, 2, 2, 5, 2, 6, 2, 2, 2, 6, 2, 2, 2, 9, 2, 2, 2, 2, 2, 2, 9, 2, 6, 2, 6, 2, 2, 6, 2, 2, 2, 6, 2, 6, 2, 2, 6, 2, 2, 2, 9, 2, 2, 6, 6, 2, 2, 8, 2, 2, 6, 2, 2, 2, 2, 2, 2, 2, 6, 2, 2, 2, 2, 2, 2, 2, 2, 2, 6, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 6, 2, 6, 2, 2, 9, 2, 2, 9, 9, 2, 9, 2, 2, 2, 6, 2, 2, 2, 9, 9, 2, 2, 2, 2, 2, 2, 2, 6, 2, 2, 2, 2, 2, 5, 6, 2, 6, 2, 2, 2, 2, 2, 2, 6, 2, 2, 2, 2, 2, 6, 2, 2, 2, 2, 2, 9, 2, 2, 2, 2, 2, 2, 6, 2, 2, 2, 2, 2, 2, 9, 2, 9, 9, 2, 2, 2, 2, 2, 6, 2, 6, 2, 6, 9, 2, 2, 2, 2, 6, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 6, 2, 2, 2, 2, 2, 2, 6, 6, 2, 2, 2, 3, 2, 2, 2, 2, 2, 2, 6, 6, 9, 9, 2, 2, 2, 2, 2, 6, 2, 2, 2, 2, 2, 6, 6, 2, 2, 2, 2, 2, 9, 2, 6, 2, 2, 2, 6, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 6, 2, 9, 5, 6, 2, 2, 2, 6, 2, 6, 2, 2, 2, 2, 2, 6, 2, 2, 2, 2, 2, 2, 2, 2, 2, 9, 6, 2, 6, 2, 6, 2, 2, 2, 2, 2, 6, 6, 2, 6, 2, 2, 2, 9, 6, 6, 9, 2, 2, 2, 2, 2, 6, 6, 2, 9, 2, 2, 2, 2, 2, 2, 2, 2, 6, 9, 9, 9, 2, 2, 2, 6, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 9, 2, 2, 2, 6, 2, 9, 2, 2, 2, 2, 2, 2, 2, 2, 2, 6, 6, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 6, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 6, 2, 2, 2, 2, 2, 2, 6, 2, 2, 2, 2, 2, 6, 2, 2, 2, 2, 6, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 6, 2, 2, 2, 2, 2, 2, 2, 2, 6, 2, 2, 9, 2, 2, 2, 2, 2, 6, 2, 2, 6, 2, 6, 2, 2, 2, 2, 2, 6, 2, 9, 2, 2, 2, 2, 2, 2, 2, 2, 6, 2, 5, 2, 2, 2, 2, 2, 6, 9, 9, 2, 9, 2, 6, 2, 2, 2, 2, 2, 2, 2, 2, 2, 9, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 6, 2, 2, 2, 2, 2, 6, 6, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 6, 6, 2, 2, 2, 2, 9, 2, 2, 2, 9, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 6, 2, 2, 2, 9, 2, 9, 6, 2, 2, 2, 6, 6, 2, 2, 6, 2, 6, 2, 2, 2, 2, 2, 2, 2, 2, 2, 6, 6, 2, 2, 9, 2, 2, 2, 9, 2, 2, 2, 9, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 6, 2, 2, 9, 2, 2, 2, 2, 2, 6, 2, 9, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 6, 6, 2, 2, 9, 2, 2, 2, 2, 2, 2, 2, 2, 6, 2, 6, 2, 2, 2, 2, 2, 2, 2, 9, 2, 2, 2, 2, 2, 6, 2, 6, 2, 2, 2, 2, 2, 9, 6, 6, 5, 6, 9, 2, 9, 6, 2, 2, 2, 6, 2, 9, 6, 6, 2, 2, 2, 2, 2, 2, 2, 9, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 9, 2, 6, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 9, 2, 9, 2, 9, 2, 2, 9, 2, 9, 9, 2, 2, 9, 2, 2, 2, 2, 2, 2, 6, 2, 6, 2, 2, 2, 2, 2, 6, 2, 2, 2, 2, 2, 6, 2, 2, 2, 2, 9, 2, 2, 6, 2, 6, 2, 2, 2, 2, 2, 2, 6, 2, 6, 2, 2, 2, 6, 2, 2, 9, 9, 6, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 9, 2, 2, 2, 2, 2, 2, 2, 2, 6, 2, 2, 2, 2, 2, 9, 2, 6, 9, 2, 9, 9, 2, 6, 6, 2, 6, 2, 2, 2, 2, 2, 2, 6, 2, 2, 2, 2, 2, 2, 2, 9, 2, 2, 2, 2, 6, 2, 2, 2, 6, 2, 2, 6, 6, 2, 2, 2, 9, 2, 2, 2, 2, 2, 2, 2, 6, 2, 2, 2, 2, 6, 2, 2, 2, 6, 2, 6, 2, 2, 2, 2, 6, 6, 2, 2, 2, 6, 2, 2, 2, 9, 2, 2, 9, 2, 2, 2, 2, 6, 2, 2, 2, 2, 2, 2, 6, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 6, 2, 2, 2, 6, 2, 6, 2, 6, 6, 2, 2, 2, 2, 6, 2, 2, 2, 2, 2, 2, 2, 2, 9, 2, 2, 2, 2, 2, 2, 6, 2, 9, 2, 2, 9, 2, 2, 2, 6, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 9, 6, 2, 2, 2, 2, 2, 2, 2, 6, 2, 2, 2, 2, 2, 2, 6, 2, 6, 2, 2, 2, 2, 2, 2, 6, 2, 2, 2, 6, 2, 2, 6, 2, 2, 6, 2, 2, 2, 2, 6, 2, 6, 9, 2, 2, 2, 6, 2, 6, 6, 2, 2, 6, 6, 2, 2, 2, 9, 2, 2, 2, 2, 2, 2, 9, 2, 2, 2, 2, 2, 2, 6, 2, 2, 9, 2, 2, 2, 2, 2, 2, 9, 6, 9, 6, 2, 2, 6, 9, 2, 2, 9, 2, 6, 2, 2, 2, 2, 6, 2, 2, 2, 2, 2, 2, 2, 2, 6, 9, 9, 9, 2, 2, 6, 2, 2, 6, 2, 6, 2, 2, 2, 2, 2, 9, 2, 2, 2, 2, 2, 2, 2, 2, 6, 9, 2, 2, 2, 2, 6, 2, 2, 6, 2, 2, 6, 9, 6, 2, 2, 2, 6, 2, 2, 9, 2, 2, 2, 6, 6, 2, 6, 6, 9, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 6, 2, 2, 6, 2, 2, 9, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 6, 2, 2, 2, 6, 6, 2, 2, 2, 2, 2, 2, 2, 2, 6, 6, 2, 2, 2, 2, 2, 6, 6, 2, 9, 2, 9, 6, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 9, 5, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 6, 8, 2, 9, 6, 2, 2, 2, 2, 2, 6, 2, 2, 6, 9, 2, 9, 2, 2, 2, 2, 2, 2, 2, 2, 9, 2, 2, 2, 6, 2, 6, 6, 2, 2, 2, 2, 2, 2, 2, 2, 2, 9, 9, 6, 2, 9, 9, 2, 6, 2, 9, 6, 2, 2, 2, 2, 6, 2, 9, 2, 2, 2, 2, 2, 6, 2, 2, 2, 2, 2, 6, 2, 2, 2, 6, 2, 2, 2, 6, 2, 6, 2, 2, 2, 2, 2, 2, 2, 2, 6, 2, 2, 2, 2, 2, 2, 2, 9, 2, 2, 2, 2, 2, 2, 2, 9, 5, 2, 9, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 9, 2, 2, 2, 9, 2, 2, 2, 2, 2, 2, 9, 6, 6, 9, 2, 2, 9, 2, 2, 2, 2, 2, 6, 2, 2, 6, 2, 6, 2, 2, 2, 2, 2, 2, 9, 2, 2, 2, 2, 9, 2, 2, 6, 9, 2, 6, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 9, 2, 6, 2, 2, 2, 2, 2, 6, 2, 2, 2, 2, 2, 2, 9, 6, 2, 9, 2, 2, 2, 9, 2, 2, 2, 9, 2, 2, 2, 2, 2, 2, 2, 2, 2, 6, 6, 2, 5, 9, 2, 2, 2, 6, 9, 9, 2, 2, 6, 2, 2, 9, 6, 2, 2, 2, 2, 2, 9, 2, 2, 2, 9, 2, 2, 9, 6, 6, 2, 2, 2, 2, 2, 2, 6, 2, 2, 2, 2, 9, 2, 2, 2, 2, 2, 2, 2, 2, 2, 9, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 6, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 9, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 6, 2, 2, 9, 2, 2, 2, 2, 2, 2, 2, 2, 2, 6, 2, 2, 2, 6, 2, 2, 9, 2, 2, 6, 2, 2, 2, 2, 2, 2, 2, 6, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 5, 2, 6, 2, 2, 2, 2, 2, 2, 2, 9, 2, 9, 2, 2, 2, 6, 2, 6, 2, 2, 2, 2, 2, 9, 2, 2, 2, 2, 2, 2, 2, 2, 6, 2, 2, 2, 2, 6, 2, 9, 9, 9, 2, 2, 2, 2, 2, 2, 2, 6, 6, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 9, 9, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 9, 2, 2, 2, 2, 2, 6, 2, 9, 2, 2, 6, 9, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 5, 2, 2, 2, 2, 2, 2, 2, 6, 2, 2, 6, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 9, 2, 2, 2, 2, 2, 2, 5, 2, 2, 6, 2, 9, 9, 2, 2, 9, 9, 2, 2, 2, 6, 2, 2, 2, 2, 6, 2, 2, 2, 2, 2, 2, 2, 2, 2, 6, 2, 5, 2, 2, 2, 6, 2, 2, 2, 2, 2, 2, 9, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 9, 2, 2, 2, 2, 2, 2, 2, 9, 9, 2, 2, 2, 9, 2, 2, 2, 2, 6, 2, 2, 2, 2, 2, 2, 2, 2, 2, 6, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 5, 9, 2, 2, 2, 2, 2, 9, 2, 2, 5, 2, 6, 2, 9, 2, 2, 2, 2, 9, 6, 2, 2, 9, 2, 6, 2, 2, 2, 2, 2, 2, 2, 9, 2, 2, 2, 2, 9, 6, 2, 2, 2, 2, 2, 9, 2, 2, 2, 6, 2, 2, 6, 2, 2, 2, 2, 2, 2, 2, 9, 2, 2, 2, 2, 9, 6, 2, 2, 6, 2, 2, 2, 2, 9, 2, 2, 2, 2, 2, 9, 2, 2, 2, 6, 2, 2, 2, 2, 2, 2, 2, 2, 6, 2, 2, 2, 2, 2, 2, 9, 2, 2, 9, 2, 2, 2, 6, 2, 6, 2, 2, 6, 2, 2, 2, 2, 9, 9, 6, 2, 6, 2, 2, 2, 2, 6, 6, 2, 2, 2, 9, 2, 2, 2, 6, 2, 9, 2, 9, 2, 2, 2, 2, 2, 9, 9, 2, 2, 9, 2, 2, 2, 6, 2, 2, 2, 2, 2, 2, 9, 9, 2, 2, 2, 9, 2, 2, 2, 2, 5, 2, 2, 6, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 9, 2, 2, 2, 2, 2, 2, 2, 6, 2, 2, 9, 2, 6, 2, 9, 2, 2, 9, 2, 2, 6, 2, 2, 2, 2, 6, 2, 2, 2, 2, 2, 2, 2, 6, 2, 2, 2, 2, 2, 2, 2, 2, 6, 5, 9, 2, 2, 2, 6, 2, 2, 6, 2, 2, 6, 9, 2, 2, 6, 2, 2, 2, 2, 9, 2, 2, 2, 2, 2, 2, 2, 2, 9, 9, 2, 2, 2, 2, 2, 2, 9, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 9, 2, 9, 2, 2, 2, 2, 2, 6, 2, 2, 2, 2, 6, 2, 2, 2, 2, 2, 2, 2, 6, 2, 2, 2, 2, 6, 2, 2, 6, 2, 2, 2, 2, 2, 2, 2, 6, 2, 2, 6, 2, 9, 9, 2, 5, 2, 9, 2, 9, 2, 2, 9, 2, 2, 2, 2, 2, 2, 2, 2, 9, 2, 6, 2, 6, 6, 6, 2, 6, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 6, 2, 2, 2, 2, 2, 2, 2, 2, 2, 9, 2, 2, 2, 6, 9, 2, 2, 2, 2, 2, 2, 9, 2, 2, 6, 2, 2, 2, 6, 2, 2, 2, 2, 6, 3, 2, 2, 2, 6, 2, 6, 2, 9, 8, 9, 2, 6, 2, 2, 2, 2, 2, 2, 2, 2, 6, 6, 2, 9, 2, 2, 2, 9, 6, 2, 9, 9, 2, 2, 2, 2, 6, 2, 6, 2, 2, 9, 2, 2, 9, 2, 2, 2, 9, 2, 2, 2, 2, 9, 2, 2, 2, 2, 2, 2, 2, 2, 9, 2, 2, 2, 2, 6, 2, 2, 2, 2, 2, 9, 2, 2, 2, 6, 2, 2, 2, 2, 2, 6, 2, 2, 9, 2, 2, 2, 9, 6, 2, 2, 6, 2, 2, 6, 2, 2, 2, 2, 2, 2, 6, 2, 2, 6, 9, 2, 6, 2, 2, 2, 2, 2, 2, 2, 9, 9, 2, 2, 2, 2, 2, 2, 6, 2, 2, 5, 2, 2, 2, 2, 2, 2, 6, 2, 6, 2, 2, 2, 2, 2, 9, 6, 6, 6, 9, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 6, 2, 2, 6, 9, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 5, 2, 2, 2, 2, 2, 6, 2, 9, 2, 6, 5, 2, 2, 6, 2, 2, 6, 9, 9, 2, 9, 6, 6, 2, 6, 2, 2, 2, 2, 2, 2, 2, 2, 6, 9, 2, 2, 2, 2, 2, 6, 2, 2, 6, 2, 6, 9, 6, 2, 6, 2, 6, 2, 9, 2, 2, 2, 2, 2, 2, 2, 2, 6, 2, 2, 6, 2, 9, 6, 2, 2, 5, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 9, 9, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 6, 2, 8, 2, 2, 2, 2, 2, 9, 2, 2, 2, 2, 9, 9, 2, 2, 2, 2, 2, 9, 9, 6, 2, 5, 2, 2, 6, 2, 2, 2, 2, 9, 2, 6, 2, 2, 2, 6, 2, 2, 6, 2, 9, 6, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 6, 6, 2, 2, 2, 6, 2, 2, 2, 6, 2, 2, 2, 2, 2, 2, 6, 2, 6, 2, 2, 2, 2, 9, 6, 6, 6, 3, 2, 2, 2, 2, 9, 2, 2, 2, 2, 9, 2, 2, 2, 6, 2, 2, 2, 2, 2, 2, 2, 9, 2, 2, 2, 2, 2, 2, 2, 2, 2, 6, 2, 2, 2, 9, 2, 2, 2, 6, 9, 2, 2, 2, 2, 9, 2, 9, 2, 2, 2, 9, 2, 9, 2, 2, 6, 2, 9, 9, 2, 2, 6, 2, 2, 2, 2, 2, 2, 2, 6, 2, 2, 2, 6, 2, 9, 5, 2, 2, 5, 6, 2, 6, 2, 9, 2, 9, 2, 2, 2, 2, 2, 2, 6, 6, 2, 2, 2, 9, 6, 2, 2, 2, 9, 2, 6, 9, 2, 2, 2, 6, 2, 2, 2, 9, 2, 9, 2, 2, 2, 6, 2, 2, 2, 2, 9, 2, 2, 6, 6, 9, 6, 2, 2, 2, 2, 2, 2, 2, 2, 2, 9, 6, 6, 6, 2, 9, 2, 2, 2, 9, 2, 2, 6, 2, 2, 6, 2, 2, 2, 2, 6, 2, 9, 6, 2, 2, 2, 2, 2, 2, 6, 2, 6, 2, 2, 2, 9, 9, 6, 2, 2, 2, 6, 6, 2, 6, 9, 2, 6, 2, 2, 2, 2, 2, 2, 6, 2, 6, 9, 6, 2, 2, 6, 6, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 9, 2, 2, 2, 2, 2, 9, 2, 2, 2, 2, 6, 2, 9, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 9, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 6, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 5, 6, 2, 2, 6, 2, 2, 2, 6, 2, 2, 2, 6, 2, 2, 6, 2, 5, 2, 6, 2, 2, 2, 2, 2, 2, 2, 6, 2, 2, 2, 2, 2, 2, 9, 6, 2, 5, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 6, 2, 2, 2, 2, 2, 2, 6, 9, 2, 2, 6, 2, 2, 2, 2, 2, 9, 2, 2, 6, 2, 2, 2, 2, 2, 2, 9, 2, 2, 2, 6, 2, 2, 6, 6, 2, 9, 2, 6, 6, 9, 2, 2, 2, 9, 2, 6, 6, 2, 2, 2, 2, 6, 2, 2, 6, 2, 2, 2, 2, 2, 9, 6, 6, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 9, 2, 2, 2, 2, 6, 9, 2, 2, 2, 2, 2, 9, 2, 2, 6, 6, 2, 2, 2, 2, 9, 2, 2, 2, 2, 9, 2, 9, 2, 6, 2, 2, 2, 2, 2, 6, 6, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 6, 6, 2, 2, 6, 2, 6, 2, 2, 6, 2, 2, 2, 2, 9, 6, 2, 2, 2, 6, 2, 2, 2, 2, 9, 2, 2, 2, 2, 2, 2, 6, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 6, 3, 6, 2, 6, 2, 2, 2, 2, 2, 2, 2, 2, 6, 2, 2, 2, 2, 6, 2, 9, 2, 2, 2, 2, 2, 9, 2, 2, 9, 2, 9, 2, 2, 2, 2, 2, 2, 2, 6, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 9, 2, 6, 9, 2, 2, 2, 2, 2, 2, 2, 2, 2, 9, 2, 2, 2, 9, 5, 9, 9, 6, 2, 2, 2, 2, 2, 6, 2, 2, 2, 6, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 6, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 6, 6, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 6, 2, 2, 2, 2, 2, 9, 9, 6, 2, 2, 6, 2, 2, 6, 2, 2, 6, 2, 6, 2, 2, 2, 2, 2, 6, 2, 9, 6, 6, 2, 2, 2, 2, 6, 2, 2, 2, 2, 2, 2, 6, 6, 2, 2, 2, 2, 2, 6, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 6, 2, 2, 2, 2, 2, 6, 2, 6, 2, 9, 2, 2, 6, 2, 2, 9, 2, 2, 2, 6, 2, 2, 2, 2, 2, 2, 2, 9, 6, 2, 2, 6, 2, 2, 2, 2, 2, 2, 9, 2, 2, 2, 2, 2, 9, 2, 2, 2, 6, 2, 2, 9, 6, 2, 2, 2, 2, 2, 2, 9, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 6, 9, 2, 6, 2, 2, 6, 6, 2, 2, 6, 2, 2, 2, 2, 2, 2, 2, 9, 2, 2, 2, 2, 6, 2, 2, 6, 2, 2, 9, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 6, 6, 6, 2, 2, 2, 6, 2, 2, 2, 2, 9, 2, 2, 6, 2, 2, 9, 2, 2, 2, 2, 9, 2, 2, 5, 6, 2, 2, 6, 2, 2, 2, 2, 2, 6, 9, 3, 2, 2, 2, 2, 2, 2, 2, 2, 6, 2, 2, 2, 6, 2, 2, 2, 3, 2, 2, 2, 2, 2, 6, 2, 2, 9, 2, 6, 2, 2, 2, 2, 2, 6, 5, 2, 2, 2, 2, 6, 2, 2, 2, 2, 2, 0, 6, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 9, 2, 2, 9, 2, 2, 2, 2, 2, 2, 2, 2, 6, 2, 2, 6, 3, 2, 2, 2, 2, 2, 2, 9, 2, 2, 2, 9, 6, 2, 2, 2, 2, 2, 6, 9, 2, 2, 2, 6, 2, 2, 2, 2, 2, 9, 9, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 6, 2, 2, 9, 2, 2, 2, 9, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 6, 2, 2, 6, 2, 9, 2, 2, 2, 2, 2, 6, 9, 9, 2, 2, 9, 2, 6, 9, 2, 2, 9, 2, 2, 2, 2, 2, 6, 2, 2, 2, 2, 6, 2, 2, 9, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 9, 2, 2, 2, 2, 6, 2, 2, 2, 6, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 9, 2, 2, 2, 2, 9, 6, 9, 2, 2, 2, 9, 9, 2, 2, 6, 9, 2, 9, 2, 5, 6, 2, 2, 6, 2, 2, 6, 2, 2, 2, 2, 2, 6, 2, 6, 2, 2, 2, 6, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 9, 2, 2, 9, 2, 2, 2, 2, 9, 2, 2, 6, 2, 2, 2, 5, 9, 2, 6, 2, 2, 2, 2, 2, 2, 6, 2, 2, 2, 2, 2, 2, 2, 6, 2, 2, 2, 2, 2, 9, 2, 2, 2, 2, 2, 2, 2, 2, 2, 5, 5, 2, 2, 2, 2, 6, 2, 2, 6, 2, 2, 2, 2, 2, 2, 2, 2, 2, 9, 2, 6, 2, 9, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 6, 6, 2, 2, 5, 2, 2, 2, 6, 2, 2, 2, 2, 2, 2, 2, 2, 2, 5, 2, 2, 2, 2, 2, 2, 2, 2, 6, 2, 2, 2, 2, 2, 2, 2, 6, 6, 2, 2, 2, 2, 2, 2, 2, 6, 2, 2, 6, 2, 6, 6, 2, 2, 2, 2, 2, 2, 6, 2, 6, 2, 6, 2, 2, 2, 6, 2, 6, 2, 2, 2, 2, 9, 2, 2, 2, 9, 9, 2, 2, 2, 6, 2, 2, 2, 2, 2, 2, 2, 6, 2, 2, 2, 2, 2, 6, 2, 2, 6, 2, 9, 2, 2, 2, 2, 2, 2, 2, 6, 2, 2, 2, 2, 2, 9, 2, 6, 2, 2, 2, 2, 9, 2, 2, 6, 9, 2, 2, 9, 2, 2, 6, 2, 9, 2, 9, 2, 2, 6, 2, 2, 2, 6, 2, 2, 2, 6, 2, 2, 2, 2, 2, 6, 2, 2, 6, 2, 9, 2, 9, 9, 2, 2, 2, 6, 9, 6, 2, 2, 2, 2, 2, 6, 2, 2, 2, 2, 6, 2, 2, 2, 2, 2, 6, 2, 2, 2, 2, 2, 2, 2, 9, 2, 2, 6, 2, 6, 2, 2, 2, 2, 2, 2, 2, 6, 2, 2, 6, 2, 2, 6, 2, 2, 2, 2, 6, 2, 2, 2, 2, 2, 2, 2, 2, 2, 5, 2, 2, 2, 2, 9, 2, 2, 2, 6, 2, 9, 2, 9, 2, 2, 9, 2, 2, 2, 2, 6, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 6, 2, 2, 2, 2, 2, 2, 9, 6, 6, 2, 6, 2, 2, 2, 6, 2, 2, 2, 6, 2, 2, 6, 2, 6, 2, 2, 2, 2, 6, 2, 6, 2, 2, 2, 9, 5, 2, 6, 2, 2, 2, 2, 6, 2, 2, 2, 2, 6, 9, 2, 6, 2, 2, 2, 2, 2, 6, 2, 9, 2, 6, 2, 6, 2, 2, 6, 2, 9, 2, 2, 2, 2, 2, 2, 9, 6, 2, 2, 2, 2, 2, 2, 2, 6, 9, 2, 9, 9, 2, 2, 2, 2, 2, 2, 6, 2, 2, 9, 2, 2, 2, 2, 2, 2, 9, 6, 2, 6, 2, 2, 2, 2, 2, 2, 6, 6, 2, 2, 2, 2, 2, 9, 9, 9, 2, 2, 9, 2, 9, 6, 9, 2, 2, 6, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 9, 2, 2, 2, 2, 2, 6, 2, 9, 9, 2, 6, 2, 6, 2, 2, 2, 2, 2, 6, 2, 2, 9, 2, 2, 2, 6, 2, 9, 2, 2, 6, 2, 2, 6, 9, 2, 9, 2, 6, 2, 2, 6, 2, 9, 9, 6, 6, 2, 6, 2, 2, 2, 2, 6, 2, 9, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 6, 2, 2, 9, 2, 2, 6, 2, 6, 2, 2, 2, 9, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 6, 6, 2, 6, 2, 2, 2, 2, 2, 2, 9, 2, 2, 2, 2, 2, 5, 2, 2, 9, 6, 2, 2, 2, 2, 2, 2, 2, 2, 2, 9, 9, 2, 2, 2, 2, 2, 2, 6, 2, 2, 2, 2, 2, 9, 2, 2, 6, 2, 9, 2, 2, 9, 2, 2, 2, 6, 2, 9, 2, 2, 2, 2, 9, 2, 9, 2, 9, 2, 2, 2, 2, 2, 2, 6, 6, 2, 9, 6, 2, 2, 2, 2, 2, 2, 6, 6, 2, 2, 9, 2, 2, 2, 2, 2, 6, 2, 6, 2, 2, 2, 6, 2, 2, 2, 2, 2, 2, 6, 6, 9, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 9, 2, 2, 2, 2, 6, 2, 9, 2, 2, 2, 2, 9, 2, 2, 2, 2, 2, 2, 2, 2, 2, 6, 2, 2, 6, 6, 2, 9, 2, 2, 2, 6, 6, 2, 2, 2, 2, 9, 2, 2, 2, 2, 6, 2, 2, 2, 5, 2, 9, 2, 2, 2, 6, 9, 2, 2, 2, 2, 2, 2, 9, 2, 2, 6, 2, 2, 2, 6, 2, 9, 2, 2, 2, 6, 2, 2, 2, 2, 2, 9, 6, 9, 2, 6, 2, 2, 6, 6, 2, 9, 2, 6, 2, 6, 6, 2, 2, 2, 2, 2, 6, 6, 2, 2, 2, 2, 2, 6, 2, 2, 2, 6, 2, 2, 2, 2, 9, 2, 2, 6, 2, 2, 6, 2, 2, 2, 2, 2, 2, 2, 2, 2, 6, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 6, 2, 2, 2, 9, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 9, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 6, 2, 2, 2, 6, 2, 2, 2, 2, 6, 2, 2, 2, 2, 2, 2, 2, 2, 2, 9, 2, 9, 2, 2, 9, 2, 2, 2, 2, 2, 2, 2, 2, 6, 2, 2, 6, 6, 2, 2, 2, 2, 2, 2, 6, 4, 2, 2, 2, 6, 2, 2, 9, 6, 6, 6, 2, 2, 2, 2, 2, 2, 2, 2, 2, 9, 2, 2, 6, 2, 9, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 9, 2, 2, 8, 2, 9, 2, 5, 6, 2, 2, 2, 2, 6, 2, 9, 2, 9, 2, 2, 2, 2, 2, 6, 2, 2, 2, 2, 6, 6, 2, 2, 2, 6, 2, 9, 2, 2, 2, 9, 2, 2, 2, 2, 2, 6, 2, 2, 2, 9, 2, 2, 2, 2, 9, 2, 2, 6, 2, 2, 2, 2, 2, 2, 2, 6, 2, 6, 2, 9, 6, 2, 6, 2, 9, 6, 2, 2, 2, 2, 6, 2, 2, 2, 2, 9, 2, 2, 9, 6, 6, 2, 2, 2, 6, 2, 6, 2, 9, 2, 2, 2, 2, 2, 9, 2, 2, 2, 2, 2, 2, 2, 9, 6, 2, 2, 9, 9, 2, 6, 2, 2, 6, 2, 9, 2, 2, 9, 2, 2, 6, 8, 6, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 5, 2, 2, 2, 2, 2, 5, 6, 2, 9, 9, 2, 2, 6, 2, 2, 2, 2, 2, 2, 2, 2, 2, 6, 2, 5, 2, 9, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 6, 9, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 6, 2, 2, 2, 6, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 9, 2, 6, 2, 6, 2, 2, 2, 2, 2, 2, 2, 9, 2, 9, 2, 2, 2, 2, 2, 2, 2, 2, 6, 2, 2, 2, 2, 6, 2, 2, 2, 6, 6, 2, 2, 9, 2, 9, 2, 2, 2, 2, 9, 6, 2, 2, 9, 2, 6, 2, 2, 6, 6, 2, 2, 2, 2, 2, 6, 2, 2, 2, 9, 2, 2, 2, 9, 2, 2, 2, 2, 6, 2, 6, 2, 2, 2, 2, 2, 9, 2, 2, 2, 9, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 6, 9, 6, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 6, 2, 9, 9, 9, 9, 2, 9, 2, 2, 6, 9, 2, 9, 6, 2, 9, 2, 2, 6, 2, 9, 2, 2, 2, 2, 2, 2, 6, 2, 9, 9, 2, 2, 2, 9, 6, 6, 9, 2, 2, 2, 2, 2, 9, 9, 6, 2, 2, 9, 2, 2, 9, 6, 2, 2, 2, 2, 6, 9, 2, 2, 2, 2, 2, 2, 2, 2, 6, 2, 2, 2, 6, 6, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 6, 2, 2, 3, 2, 6, 6, 2, 2, 2, 2, 2, 2, 9, 6, 9, 9, 2, 9, 2, 2, 2, 2, 6, 2, 2, 2, 2, 6, 6, 2, 2, 6, 2, 2, 2, 9, 6, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 6, 2, 2, 9, 6, 9, 2, 2, 9, 2, 2, 2, 2, 2, 9, 2, 2, 2, 2, 2, 2, 6, 6, 2, 6, 2, 6, 2, 9, 2, 2, 2, 2, 2, 2, 2, 2, 6, 6, 9, 9, 2, 2, 2, 2, 2, 2, 3, 2, 2, 2, 2, 2, 2, 2, 2, 6, 6, 6, 2, 9, 2, 9, 2, 2, 2, 6, 2, 9, 6, 2, 2, 6, 2, 6, 2, 6, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 6, 2, 2, 2, 2, 2, 2, 2, 2, 2, 9, 2, 2, 6, 2, 2, 2, 9, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 9, 2, 2, 2, 9, 2, 6, 2, 2, 2, 9, 2, 2, 2, 2, 2, 2, 2, 2, 6, 2, 6, 2, 2, 2, 2, 2, 2, 2, 9, 2, 9, 2, 6, 9, 2, 6, 2, 9, 2, 2, 2, 2, 2, 2, 2, 2, 6, 9, 2, 2, 6, 9, 3, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 9, 2, 9, 2, 6, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 9, 2, 9, 6, 2, 6, 2, 9, 2, 2, 2, 6, 2, 2, 2, 2, 6, 2, 2, 6, 2, 2, 2, 2, 2, 2, 9, 2, 2, 2, 2, 2, 2, 2, 8, 8, 6, 2, 9, 2, 6, 2, 2, 6, 2, 2, 9, 2, 2, 6, 2, 2, 2, 2, 2, 2, 2, 2, 9, 9, 9, 2, 2, 2, 2, 2, 2, 2, 2, 2, 9, 9, 2, 2, 2, 2, 2, 2, 2, 2, 6, 2, 2, 9, 9, 6, 2, 6, 2, 6, 6, 6, 2, 2, 6, 9, 2, 6, 2, 2, 9, 2, 2, 6, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 9, 2, 2, 2, 2, 2, 2, 2, 6, 2, 2, 2, 2, 2, 2, 9, 2, 9, 2, 2, 2, 2, 2, 2, 2, 6, 2, 2, 2, 2, 2, 6, 2, 2, 2, 2, 2, 2, 6, 2, 2, 2, 2, 2, 2, 2, 2, 3, 2, 2, 2, 2, 6, 6, 8, 2, 2, 2, 6, 2, 2, 2, 2, 6, 2, 2, 2, 2, 9, 2, 2, 2, 2, 2, 2, 2, 6, 2, 2, 2, 2, 2, 2, 2, 9, 2, 2, 2, 9, 2, 2, 9, 2, 2, 6, 9, 2, 9, 2, 2, 2, 6, 2, 2, 2, 2, 2, 2, 2, 2, 9, 2, 2, 2, 2, 2, 6, 2, 6, 2, 2, 2, 6, 2, 2, 9, 9, 2, 9, 2, 6, 2, 2, 2, 6, 2, 2, 2, 6, 2, 2, 6, 2, 2, 2, 2, 2, 2, 2, 2, 9, 2, 2, 2, 2, 6, 2, 2, 2, 2, 2, 2, 2, 9, 2, 2, 2, 2, 2, 6, 6, 2, 2, 2, 2, 2, 6, 2, 2, 9, 2, 6, 2, 2, 2, 2, 6, 2, 2, 6, 2, 2, 2, 2, 2, 2, 2, 9, 2, 9, 2, 2, 2, 6, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 9, 2, 6, 2, 2, 2, 2, 6, 6, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 6, 2, 2, 9, 6, 2, 6, 2, 2, 6, 6, 2, 2, 6, 2, 2, 2, 2, 2, 9, 2, 2, 2, 9, 2, 2, 9, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 6, 6, 2, 6, 2, 2, 2, 2, 6, 2, 6, 2, 9, 2, 9, 2, 9, 2, 2, 6, 2, 2, 2, 2, 2, 9, 6, 6, 2, 2, 6, 2, 2, 2, 2, 2, 2, 6, 2, 2, 2, 2, 2, 2, 9, 2, 2, 9, 2, 2, 2, 6, 2, 2, 6, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 9, 2, 2, 2, 2, 2, 2, 9, 2, 2, 6, 2, 6, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 6, 2, 9, 6, 6, 2, 2, 9, 2, 6, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 9, 2, 2, 6, 2, 6, 2, 2, 2, 2, 2, 2, 2, 2, 9, 9, 6, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 9, 2, 2, 2, 2, 6, 6, 2, 2, 2, 2, 2, 9, 2, 2, 6, 6, 2, 2, 2, 9, 2, 2, 2, 2, 2, 2, 2, 2, 2, 6, 9, 2, 2, 2, 2, 2, 2, 2, 6, 2, 8, 2, 2, 6, 2, 2, 2, 2, 9, 9, 2, 2, 2, 2, 9, 2, 2, 2, 2, 2, 2, 2, 2, 9, 2, 2, 2, 2, 2, 2, 2, 2, 6, 2, 2, 2, 2, 6, 2, 2, 2, 6, 9, 2, 2, 2, 2, 6, 2, 9, 2, 2, 2, 2, 2, 2, 2, 6, 2, 2, 9, 2, 2, 2, 2, 2, 2, 2, 6, 2, 2, 2, 6, 2, 2, 2, 2, 2, 6, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 6, 2, 2, 6, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 6, 6, 2, 2, 6, 6, 2, 2, 2, 2, 2, 9, 6, 2, 2, 6, 2, 9, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 6, 2, 2, 2, 2, 9, 2, 2, 9, 2, 9, 2, 2, 2, 2, 2, 9, 2, 6, 9, 2, 2, 2, 6, 2, 2, 2, 2, 9, 2, 2, 2, 6, 2, 2, 2, 2, 6, 2, 2, 2, 2, 2, 6, 2, 2, 2, 2, 2, 2, 2, 2, 2, 6, 2, 2, 6, 2, 2, 2, 6, 2, 9, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 6, 6, 2, 5, 2, 2, 2, 2, 2, 2, 2, 2, 2, 9, 2, 2, 2, 2, 2, 2, 9, 2, 9, 2, 2, 2, 2, 6, 2, 6, 2, 2, 6, 2, 2, 2, 2, 2, 2, 2, 2, 2, 6, 6, 6, 2, 2, 2, 2, 6, 2, 2, 2, 6, 2, 2, 2, 2, 2, 6, 6, 2, 2, 2, 2, 2, 2, 2, 6, 2, 2, 2, 2, 6, 2, 6, 6, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 9, 2, 2, 2, 5, 2, 2, 2, 2, 2, 2, 2, 9, 2, 2, 2, 2, 9, 2, 2, 2, 2, 9, 2, 6, 2, 2, 2, 9, 2, 6, 6, 2, 6, 2, 2, 2, 2, 2, 2, 6, 6, 2, 5, 6, 2, 2, 2, 9, 2, 2, 2, 2, 6, 9, 6, 2, 2, 2, 2, 2, 2, 2, 6, 6, 2, 2, 2, 2, 2, 2, 2, 9, 2, 2, 2, 2, 9, 2, 2, 2, 2, 2, 9, 6, 2, 6, 2, 2, 6, 2, 2, 6, 2, 6, 2, 9, 2, 2, 6, 2, 2, 6, 2, 2, 6, 6, 2, 2, 2, 2, 2, 6, 9, 2, 2, 2, 2, 2, 9, 9, 2, 6, 2, 2, 2, 2, 6, 2, 2, 2, 2, 2, 2, 9, 2, 2, 2, 2, 9, 2, 6, 2, 2, 2, 2, 2, 2, 2, 6, 6, 2, 2, 6, 2, 2, 2, 2, 9, 2, 2, 6, 2, 9, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 5, 2, 6, 2, 2, 9, 6, 2, 9, 2, 9, 2, 2, 2, 2, 2, 6, 2, 6, 2, 2, 2, 2, 2, 2, 2, 2, 2, 9, 2, 6, 2, 2, 6, 9, 2, 2, 2, 2, 2, 9, 2, 6, 2, 2, 2, 6, 9, 2, 2, 9, 9, 2, 2, 2, 6, 2, 2, 6, 9, 6, 2, 6, 6, 2, 2, 9, 6, 6, 6, 2, 2, 6, 6, 2, 2, 9, 2, 2, 2, 2, 9, 2, 2, 9, 2, 2, 2, 6, 9, 2, 9, 2, 2, 2, 6, 6, 2, 2, 6, 2, 6, 6, 9, 6, 2, 6, 2, 9, 2, 2, 2, 2, 2, 6, 2, 6, 2, 2, 2, 9, 2, 6, 6, 2, 2, 6, 2, 6, 2, 2, 2, 2, 2, 2, 2, 6, 9, 9, 2, 2, 2, 9, 8, 9, 9, 6, 9, 2, 2, 2, 9, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 6, 2, 6, 2, 2, 2, 6, 2, 2, 6, 2, 2, 2, 2, 2, 2, 6, 2, 2, 9, 2, 2, 6, 2, 2, 6, 9, 9, 9, 2, 2, 6, 2, 6, 2, 6, 2, 2, 2, 2, 2, 2, 9, 9, 2, 2, 2, 2, 6, 2, 6, 2, 6, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 6, 2, 2, 2, 2, 5, 2, 9, 2, 2, 2, 6, 2, 2, 2, 2, 2, 2, 2, 2, 6, 2, 6, 2, 2, 2, 2, 2, 2, 2, 2, 2, 5, 2, 2, 2, 2, 2, 2, 2, 6, 2, 6, 2, 9, 6, 2, 2, 5, 2, 2, 2, 2, 6, 2, 2, 2, 6, 2, 2, 2, 2, 6, 2, 2, 2, 2, 2, 9, 2, 6, 6, 2, 9, 2, 2, 6, 2, 2, 2, 6, 2, 2, 2, 2, 2, 9, 2, 2, 6, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 9, 2, 2, 2, 2, 2, 2, 2, 9, 2, 2, 2, 2, 2, 2, 6, 2, 2, 2, 6, 2, 2, 6, 2, 2, 6, 2, 2, 9, 2, 2, 2, 6, 2, 2, 2, 6, 2, 6, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 6, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 6, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 9, 2, 2, 2, 9, 2, 9, 9, 2, 6, 2, 2, 2, 2, 2, 6, 6, 6, 2, 2, 2, 2, 6, 2, 2, 2, 6, 2, 2, 2, 2, 2, 9, 2, 2, 2, 2, 6, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 6, 5, 2, 2, 2, 2, 2, 2, 9, 2, 5, 2, 6, 2, 9, 2, 2, 2, 2, 2, 2, 2, 2, 9, 2, 2, 2, 2, 2, 2, 9, 2, 9, 2, 2, 2, 2, 6, 2, 2, 6, 6, 2, 2, 2, 2, 2, 2, 6, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 6, 2, 2, 2, 2, 2, 6, 2, 2, 2, 2, 2, 2, 2, 2, 6, 2, 2, 2, 6, 2, 2, 9, 2, 6, 6, 2, 2, 2, 6, 2, 2, 2, 2, 2, 2, 2, 2, 6, 2, 2, 2, 2, 2, 2, 2, 2, 2, 6, 2, 6, 2, 2, 2, 2, 2, 6, 2, 2, 6, 9, 2, 9, 2, 2, 2, 2, 2, 6, 2, 6, 2, 6, 2, 2, 6, 2, 2, 2, 9, 2, 2, 9, 2, 2, 8, 2, 2, 6, 2, 2, 9, 2, 6, 2, 2, 2, 9, 2, 6, 2, 9, 9, 2, 2, 2, 2, 2, 2, 2, 6, 2, 2, 2, 2, 6, 2, 2, 2, 2, 6, 6, 8, 2, 9, 2, 2, 6, 2, 6, 2, 2, 2, 2, 2, 2, 6, 2, 2, 2, 2, 6, 2, 9, 5, 2, 2, 2, 2, 9, 2, 2, 6, 2, 2, 2, 2, 9, 2, 2, 2, 9, 9, 6, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 6, 2, 2, 9, 2, 9, 2, 2, 2, 2, 6, 2, 2, 6, 2, 2, 2, 6, 2, 2, 6, 2, 2, 2, 6, 2, 2, 2, 6, 9, 6, 2, 2, 6, 2, 2, 2, 2, 2, 2, 9, 2, 2, 2, 6, 2, 2, 9, 2, 2, 2, 2, 2, 2, 6, 2, 2, 2, 2, 2, 2, 2, 2, 6, 5, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 9, 2, 2, 2, 2, 6, 9, 2, 2, 6, 2, 2, 2, 2, 2, 2, 2, 2, 9, 2, 2, 6, 2, 9, 6, 6, 2, 2, 2, 2, 2, 6, 2, 6, 2, 2, 2, 2, 2, 9, 2, 9, 2, 6, 2, 2, 2, 9, 2, 2, 2, 2, 2, 2, 2, 2, 6, 2, 2, 2, 2, 2, 6, 2, 2, 9, 2, 2, 2, 2, 2, 2, 2, 6, 2, 2, 2, 6, 2, 9, 2, 2, 6, 2, 9, 5, 2, 6, 2, 6, 2, 2, 6, 2, 9, 2, 2, 2, 2, 6, 2, 2, 2, 9, 2, 2, 9, 2, 2, 2, 9, 2, 2, 6, 2, 2, 6, 2, 2, 6, 2, 2, 2, 2, 6, 2, 2, 2, 6, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 6, 9, 2, 2, 2, 2, 2, 2, 2, 2, 6, 9, 2, 2, 9, 2, 2, 6, 2, 2, 2, 9, 9, 2, 2, 5, 2, 9, 2, 2, 2, 2, 2, 6, 2, 2, 2, 2, 2, 2, 2, 2, 6, 2, 2, 9, 2, 6, 2, 9, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 9, 6, 2, 2, 2, 6, 9, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 6, 2, 2, 6, 6, 2, 2, 9, 9, 2, 2, 2, 2, 2, 2, 9, 2, 2, 2, 9, 6, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 6, 2, 6, 2, 2, 9, 6, 6, 6, 6, 2, 2, 2, 6, 2, 2, 2, 2, 2, 2, 9, 2, 2, 2, 2, 2, 2, 2, 2, 2, 6, 2, 2, 9, 2, 2, 2, 9, 2, 2, 2, 2, 2, 2, 9, 2, 2, 2, 2, 2, 2, 2, 9, 6, 2, 2, 2, 2, 9, 2, 2, 2, 9, 2, 2, 2, 2, 8, 2, 2, 2, 2, 2, 2, 2, 2, 6, 2, 2, 2, 2, 2, 2, 2, 2, 6, 2, 9, 2, 2, 9, 9, 2, 2, 6, 2, 9, 2, 2, 9, 2, 2, 2, 2, 2, 2, 6, 2, 2, 2, 2, 2, 6, 2, 9, 2, 2, 2, 2, 2, 9, 6, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 9, 2, 2, 2, 6, 2, 2, 2, 2, 2, 2, 2, 2, 9, 2, 2, 2, 9, 2, 2, 2, 2, 2, 9, 2, 2, 2, 2, 2, 2, 2, 6, 2, 6, 2, 2, 2, 2, 2, 2, 2, 9, 2, 2, 6, 2, 2, 2, 2, 2, 2, 2, 2, 2, 9, 9, 2, 6, 2, 6, 2, 2, 2, 6, 2, 2, 2, 9, 2, 2, 2, 2, 2, 2, 2, 2, 2, 9, 2, 5, 2, 2, 2, 6, 2, 6, 2, 9, 9, 2, 2, 2, 2, 2, 6, 2, 2, 2, 6, 2, 2, 2, 9, 2, 2, 2, 2, 2, 2, 2, 2, 6, 2, 2, 2, 2, 2, 2, 2, 2, 9, 2, 2, 6, 2, 2, 2, 2, 2, 6, 2, 6, 2, 5, 6, 5, 2, 2, 2, 3, 2, 6, 2, 3, 2, 2, 2, 2, 2, 2, 9, 2, 2, 2, 2, 2, 2, 2, 2, 9, 2, 2, 2, 2, 2, 6, 2, 2, 2, 9, 2, 2, 2, 2, 2, 2, 6, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 6, 2, 6, 2, 9, 2, 9, 6, 6, 2, 2, 2, 2, 2, 2, 6, 6, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 6, 2, 2, 6, 5, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 6, 2, 2, 2, 2, 2, 2, 2, 2, 9, 2, 9, 2, 2, 2, 2, 2, 2, 2, 2, 6, 2, 2, 2, 2, 2, 2, 2, 6, 2, 2, 9, 2, 2, 2, 2, 2, 6, 2, 2, 2, 2, 2, 9, 6, 2, 6, 6, 2, 2, 2, 2, 2, 2, 6, 2, 2, 2, 2, 2, 2, 2, 2, 5, 9, 2, 2, 2, 2, 2, 2, 6, 2, 2, 2, 2, 2, 2, 2, 2, 2, 6, 6, 2, 2, 6, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 6, 2, 8, 2, 6, 2, 6, 6, 2, 6, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 9, 2, 2, 2, 2, 2, 2, 2, 2, 2, 6, 2, 2, 2, 6, 2, 2, 2, 2, 2, 6, 2, 6, 2, 2, 2, 2, 2, 9, 2, 6, 6, 2, 2, 2, 6, 6, 2, 2, 2, 2, 2, 9, 6, 2, 2, 9, 2, 2, 9, 2, 2, 2, 9, 2, 9, 2, 2, 2, 2, 2, 9, 6, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 6, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 9, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 9, 2, 2, 6, 2, 2, 6, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 6, 6, 2, 6, 6, 2, 2, 2, 2, 2, 9, 2, 2, 2, 2, 9, 9, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 9, 2, 2, 9, 6, 2, 6, 2, 2, 2, 2, 2, 2, 6, 2, 2, 2, 6, 6, 2, 6, 2, 6, 2, 2, 6, 2, 2, 6, 2, 2, 2, 2, 2, 2, 2, 6, 9, 2, 2, 2, 6, 6, 9, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 9, 2, 2, 2, 9, 2, 2, 9, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 9, 2, 2, 9, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 9, 2, 2, 2, 2, 2, 2, 2, 9, 2, 2, 9, 2, 2, 2, 2, 9, 2, 2, 2, 2, 2, 6, 6, 2, 2, 2, 2, 2, 2, 6, 2, 9, 2, 2, 6, 2, 2, 2, 6, 2, 2, 2, 2, 2, 2, 2, 9, 2, 2, 2, 6, 6, 2, 2, 2, 2, 2, 2, 2, 2, 5, 2, 2, 6, 2, 2, 6, 2, 2, 2, 2, 2, 9, 9, 6, 2, 2, 2, 2, 2, 2, 2, 6, 2, 2, 9, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 9, 2, 2, 6, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 9, 2, 2, 9, 2, 2, 9, 2, 2, 9, 9, 9, 9, 2, 2, 2, 2, 2, 2, 5, 2, 2, 2, 2, 6, 2, 2, 9, 2, 2, 2, 6, 2, 2, 9, 2, 6, 9, 5, 2, 6, 2, 2, 6, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 6, 2, 9, 2, 2, 2, 2, 6, 2, 2, 6, 2, 2, 6, 2, 2, 2, 2, 2, 2, 2, 9, 2, 6, 6, 2, 6, 2, 2, 2, 2, 6, 2, 2, 2, 5, 9, 2, 2, 9, 6, 2, 2, 2, 2, 6, 2, 2, 2, 2, 2, 6, 2, 2, 2, 9, 2, 2, 9, 2, 9, 2, 6, 2, 2, 9, 6, 9, 2, 6, 2, 2, 2, 2, 2, 2, 2, 6, 2, 2, 8, 2, 2, 2, 2, 9, 2, 2, 6, 2, 2, 6, 2, 2, 2, 2, 2, 6, 6, 9, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 6, 2, 2, 2, 2, 2, 2, 2, 9, 2, 2, 2, 2, 2, 2, 2, 9, 6, 2, 2, 2, 2, 2, 2, 2, 2, 6, 2, 6, 2, 2, 2, 2, 2, 9, 2, 2, 2, 2, 6, 2, 2, 2, 2, 2, 9, 2, 2, 2, 9, 2, 2, 6, 6, 2, 2, 2, 2, 2, 9, 2, 2, 6, 9, 9, 6, 2, 2, 2, 9, 2, 6, 2, 6, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 6, 2, 2, 2, 2, 2, 2, 9, 2, 2, 2, 2, 2, 2, 9, 2, 2, 2, 2, 2, 2, 2, 2, 9, 2, 2, 2, 2, 6, 2, 2, 9, 2, 2, 2, 2, 2, 2, 2, 2, 9, 2, 6, 2, 2, 2, 2, 9, 2, 2, 2, 2, 2, 6, 2, 2, 2, 2, 2, 2, 5, 2, 2, 5, 2, 6, 6, 6, 6, 2, 2, 6, 2, 2, 9, 2, 2, 9, 9, 2, 2, 9, 2, 2, 2, 6, 2, 2, 2, 2, 2, 2, 9, 2, 2, 2, 2, 2, 2, 6, 2, 2, 2, 2, 6, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 6, 2, 2, 2, 2, 6, 2, 9, 2, 2, 6, 2, 2, 2, 2, 2, 2, 6, 2, 9, 2, 2, 2, 2, 2, 6, 2, 2, 2, 9, 2, 2, 9, 2, 9, 2, 2, 2, 6, 9, 2, 2, 2, 2, 2, 6, 2, 2, 6, 2, 2, 2, 2, 2, 6, 2, 9, 2, 2, 2, 2, 6, 9, 2, 2, 9, 2, 2, 2, 2, 2, 2, 2, 2, 6, 2, 6, 6, 2, 2, 2, 2, 2, 6, 6, 2, 9, 2, 9, 2, 2, 6, 2, 2, 2, 2, 2, 2, 2, 2, 6, 2, 2, 2, 2, 2, 2, 2, 2, 6, 6, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 9, 2, 2, 2, 6, 6, 2, 2, 2, 2, 2, 6, 2, 2, 2, 2, 2, 2, 9, 2, 2, 2, 6, 2, 9, 2, 2, 2, 2, 2, 2, 5, 2, 2, 2, 9, 9, 9, 2, 2, 6, 2, 2, 6, 2, 2, 6, 6, 2, 6, 2, 2, 2, 2, 2, 2, 2, 2, 2, 9, 2, 9, 2, 2, 9, 2, 6, 2, 2, 2, 2, 2, 6, 2, 2, 2, 9, 5, 2, 2, 2, 2, 2, 2, 2, 6, 2, 9, 2, 6, 2, 9, 2, 2, 2, 2, 2, 2, 2, 2, 2, 6, 2, 2, 2, 2, 2, 2, 6, 9, 2, 9, 2, 2, 2, 6, 2, 2, 6, 6, 2, 2, 5, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 6, 2, 2, 2, 9, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 6, 2, 2, 9, 2, 2, 2, 8, 2, 2, 2, 2, 2, 6, 2, 2, 2, 6, 2, 2, 6, 2, 2, 2, 9, 2, 9, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 6, 2, 9, 2, 2, 6, 6, 2, 6, 6, 2, 2, 2, 2, 6, 2, 2, 2, 2, 9, 2, 2, 2, 2, 2, 2, 8, 2, 2, 2, 2, 5, 6, 2, 2, 2, 2, 2, 6, 2, 9, 2, 2, 9, 2, 2, 2, 2, 2, 6, 2, 2, 2, 6, 6, 9, 2, 2, 2, 2, 2, 6, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 6, 2, 9, 2, 9, 2, 2, 2, 2, 2, 2, 2, 9, 2, 2, 2, 2, 2, 9, 2, 2, 2, 6, 2, 2, 2, 2, 2, 2, 2, 2, 2, 6, 2, 6, 2, 2, 2, 9, 5, 2, 2, 2, 2, 2, 6, 9, 2, 6, 2, 2, 9, 2, 6, 2, 2, 2, 2, 2, 6, 2, 2, 6, 2, 2, 6, 2, 6, 2, 6, 6, 2, 2, 2, 2, 2, 2, 2, 6, 2, 2, 2, 2, 2, 2, 6, 2, 2, 2, 2, 2, 2, 2, 9, 6, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 6, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 2, 2, 6, 2, 2, 2, 2, 2, 2, 2, 2, 2, 6, 2, 2, 2, 9, 2, 9, 2, 2, 2, 6, 6, 2, 2, 3, 9, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 6, 2, 2, 9, 2, 2, 2, 2, 2, 3, 6, 2, 2, 2, 6, 9, 2, 6, 2, 2, 2, 2, 2, 2, 2, 2, 2, 6, 2, 2, 2, 9, 2, 2, 2, 2, 2, 2, 9, 2, 2, 6, 2, 2, 2, 2, 6, 6, 2, 2, 2, 2, 2, 2, 2, 2]\n"},{"output_type":"execute_result","data":{"text/plain":"[2, 2, 2, 2, 2, 6, 2, 2, 2, 2, 2, 2, 2, 2, 2, 6, 2, 2, 2, 9]"},"metadata":{},"execution_count":32}],"execution_count":32}],"metadata":{"colab":{"name":"Welcome to DataCamp Workspaces.ipynb","provenance":[]},"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"editor":"DataLab"},"nbformat":4,"nbformat_minor":5}